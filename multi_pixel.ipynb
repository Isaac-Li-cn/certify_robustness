{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from util.models import MLP\n",
    "from util.dataset import load_pkl, load_mnist, load_fmnist, load_svhn\n",
    "from util.device_parser import config_visible_gpu\n",
    "from util.param_parser import DictParser, ListParser, IntListParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_init(mode, batch_size, in_dim, init_value, device):\n",
    "    '''\n",
    "    >>> initialize the bounds \\\\epsilon\n",
    "    '''\n",
    "\n",
    "    if mode.lower() in ['uniform',]:\n",
    "        var = torch.zeros([batch_size, 1], device = device, requires_grad = True)\n",
    "        var.data.fill_(init_value)\n",
    "        var_list = [var,]\n",
    "    elif mode.lower() in ['nonuniform',]:\n",
    "        var = torch.zeros([batch_size, in_dim], device = device, requires_grad = True)\n",
    "        var.data.fill_(init_value)\n",
    "        var_list = [var,]\n",
    "    else:\n",
    "        raise ValueError('Unrecognized mode: %s' % mode)\n",
    "\n",
    "    return var_list\n",
    "\n",
    "def var_calc(mode, batch_size, in_dim, var_list, device):\n",
    "\n",
    "    if mode.lower() in ['uniform',]:\n",
    "        var, = var_list\n",
    "        eps = var * var * torch.ones([batch_size, in_dim], device = device)\n",
    "    elif mode.lower() in ['nonuniform',]:\n",
    "        var, = var_list\n",
    "        eps = var * var\n",
    "    elif mode.lower() in ['asymmetric',]:\n",
    "        var1, var2 = var_list\n",
    "        eps = torch.cat((var1, var2), 0)\n",
    "    else:\n",
    "        raise ValueError('Unrecognized mode: %s' % mode)\n",
    "\n",
    "    return eps\n",
    "\n",
    "def clip_gradient(grad, length):\n",
    "    '''\n",
    "    >>> grad: tensor of shape [batch_size, in_dim]\n",
    "    >>> length: the maximum length allowed\n",
    "    '''\n",
    "    grad_norm = torch.norm(grad, dim = 1).view(-1, 1) + 1e-8\n",
    "    clipped_grad_norm = torch.clamp(grad_norm, max = length)\n",
    "\n",
    "    return grad / grad_norm * clipped_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "device_ids = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = load_mnist(batch_size = 10, dset = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (main_block): Sequential()\n",
       "  (output): FCLayer(\n",
       "    (layer): Linear(in_features=784, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(in_dim = 784, hidden_dims = [], out_dim = 10, nonlinearity = 'relu')\n",
    "model = model.cuda(device)\n",
    "ckpt = torch.load('./output/mnist.ckpt')\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 / 2\n",
      "10\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 1]' is invalid for input of size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ba463e663175>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 得到预测结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mresult_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpredict\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 这里用一个掩码来做预测了，得到了对应一个 batch 的结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mlabel_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 也是返回一个关于 label 的 mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m# Reinitialize the variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[5, 1]' is invalid for input of size 10"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "out_dim = 10\n",
    "optim = 'adam'\n",
    "learnrate = 5.\n",
    "max_iter = 400\n",
    "modes = 'nonuniform'\n",
    "delta = 1e-4\n",
    "update_dual_freq = 5\n",
    "inc_freq = 80\n",
    "inc_rate = 5.\n",
    "final_decay = 0.99\n",
    "\n",
    "for batch_idx in range(batch_size):\n",
    "\n",
    "        print('batch %d / %d' % (batch_idx, 2))\n",
    "\n",
    "        data_batch, label_batch = next(data_loader) # data_loader 里面存放了batch的大小\n",
    "        \n",
    "        print(len(data_batch))\n",
    "        \n",
    "        data_batch = data_batch.cuda(device) # 加载到gpu上\n",
    "        label_batch = label_batch.cuda(device) \n",
    "        data_batch = data_batch.view(data_batch.size(0), -1) # 按第一列的值标准化，防止出问题\n",
    "\n",
    "        logits = model(data_batch) # 对 data_batch 进行预测，得到预测结果，这里是概率\n",
    "        _, predict = torch.max(logits, dim = 1) # 得到预测结果\n",
    "        result_mask = (predict == label_batch).float() # 这里用一个掩码来做预测了，得到了对应一个 batch 的结果\n",
    "        label_mask = torch.ones([batch_size, out_dim], device = device).scatter_(dim = 1, index = label_batch.view(batch_size, 1), value = 0) # 也是返回一个关于 label 的 mask\n",
    "\n",
    "        # Reinitialize the variable\n",
    "        [p.data.fill_(init_value) for p in var_list] # 这个用法还是第一次看到，不过大概能猜到，这个意思应该是将 var_list 用 init_value 填充\n",
    "        beta = 1. # 这个 beta 给个了初始化，未来是随着迭代不断更新\n",
    "        grad_clip = None # 选择是否用这个方法，防止梯度爆炸\n",
    "        lam = torch.zeros([batch_size, out_dim], device = device, requires_grad = False)\n",
    "\n",
    "        # 选择优化器\n",
    "        optim = torch.optim.Adam(var_list, lr = learnrate)\n",
    "\n",
    "        # 开始训练\n",
    "        for iter_idx in range(max_iter):\n",
    "\n",
    "            # 首先根据变量来计算 eps\n",
    "            eps = var_calc(mode = args.mode, batch_size = args.batch_size, in_dim = args.in_dim, var_list = var_list, device = device)\n",
    "\n",
    "            low_bound, up_bound = model.bound(x = data_batch, ori_perturb_norm = norm, ori_perturb_eps = eps)\n",
    "            low_true = low_bound.gather(1, label_batch.view(-1, 1)) # 获取真实标签\n",
    "\n",
    "            err = low_true - up_bound - delta\n",
    "            err = torch.min(err, - lam / beta) * label_mask\n",
    "\n",
    "            eps_loss = - torch.sum(torch.log(eps), dim = 1)\n",
    "            err_loss = torch.sum(lam * err, dim = 1) + beta / 2. * torch.norm(err, dim = 1) ** 2\n",
    "\n",
    "            loss = torch.sum((eps_loss + err_loss) * result_mask) / torch.sum(result_mask)\n",
    "            eps_v = torch.sum(eps_loss * result_mask) / torch.sum(result_mask)\n",
    "            if iter_idx % 10 == 0:\n",
    "                print(batch_idx, iter_idx, beta, eps_v.data.cpu().numpy(), (loss - eps_v).data.cpu().numpy())\n",
    "\n",
    "            optim.zero_grad() #把梯度置0，重新算梯度\n",
    "            loss.backward()\n",
    "            # Gradient Clip\n",
    "            if grad_clip is not None:\n",
    "                for var in var_list:\n",
    "                    var.grad.data = clip_gradient(var.grad.data, length = grad_clip)\n",
    "            optim.step()\n",
    "\n",
    "            if (iter_idx + 1) % update_dual_freq == 0:\n",
    "                lam.data = lam.data + beta * err #这个不太确定，可能要改\n",
    "\n",
    "            if iter_idx + 1 > args.inc_min and (iter_idx + 1 - args.inc_min) % inc_freq == 0:\n",
    "                beta *= inc_rate\n",
    "                if grad_clip is not None:\n",
    "                    grad_clip /= np.sqrt(inc_rate)\n",
    "\n",
    "        # Small adjustment in the end\n",
    "        eps = var_calc(mode = args.mode, batch_size = args.batch_size, in_dim = args.in_dim, var_list = var_list, device = device)\n",
    "        shrink_times = 0\n",
    "        while shrink_times < 1000:\n",
    "\n",
    "            low_bound, up_bound = model.bound(x = data_batch, ori_perturb_norm = norm, ori_perturb_eps = eps)\n",
    "            low_true = low_bound.gather(1, label_batch.view(-1, 1))\n",
    "            err = low_true - up_bound - delta\n",
    "\n",
    "            err_min, _ = torch.min(err * label_mask + 1e-10, dim = 1, keepdim = True)\n",
    "            err_min = err_min * result_mask.view(-1, 1) + 1e-10\n",
    "\n",
    "            if float(torch.min(err_min).data.cpu().numpy()) > 0:\n",
    "                break\n",
    "\n",
    "            shrink_times += 1\n",
    "            err_sign = torch.sign(err_min)\n",
    "            coeff = (1. - final_decay) / 2. * err_sign + (1. + final_decay) / 2.\n",
    "            eps.data = eps.data * coeff\n",
    "\n",
    "        print('Shrink time = %d' % shrink_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch_11.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
